<img width="1017" alt="Image" src="https://github.com/user-attachments/assets/329dd827-61cc-48e2-b169-0fb586b60cdf" />

- ğŸ“„ **reward_model_with_lengthBias.py** : length biasë¥¼ ê°–ëŠ” reward modelì„ ìƒì„±
- ğŸ“„ **dpo.py** : DPO(original) í•™ìŠµ ì§„í–‰
- ğŸ“„ **dpo_mitigated.py** : DPO(mitigated) í•™ìŠµ ì§„í–‰
- ğŸ“„ **generate_preference_data.py** : DPO í•™ìŠµ ìœ„í•œ ë°ì´í„°ì…‹ ìƒì„± (original Reward Modelë¡œ ì„ í˜¸ë„ í‰ê°€)
- ğŸ“„ **generate_preference_data_mitigated.py** : DPO í•™ìŠµ ìœ„í•œ ë°ì´í„°ì…‹ ìƒì„± (mitigated Reward Modelë¡œ ì„ í˜¸ë„ í‰ê°€)
- ğŸ“„ **merge_sft.py** : adapter load ì¤‘ ì˜¤ë¥˜ í•´ê²° ìœ„í•´ sft ë¨¼ì € mergeí•˜ê¸° ìœ„í•œ ì½”ë“œ
- ğŸ“„ **rm_feature_analysis_lengthBias_mitigate.ipynb** : length biasë¥¼ ê°–ëŠ” reward modelë¡œ ì‹¤í—˜ ì§„í–‰
- ğŸ“‘ **dpo_test.ipynb** : original, mitigated DPO ëª¨ë¸ ì¶”ë¡  ë° í‰ê°€
- ğŸ“‘ **dpo_dataset_analysis.ipynb** : generate_preference_dataë¡œ ë§Œë“  ë°ì´í„° ê¸¸ì´ í¸í–¥ ë¶„ì„

- debiased_score_weights_alpha0.9.pt : mitigatedëœ ë§ˆì§€ë§‰ layer weight (alpha 0.9)
- debiased_score_weights.pt : mitigatedëœ ë§ˆì§€ë§‰ layer weight (alpha 1.0)
- dpo_preference_dataset_1000.jsonl : DPO í•™ìŠµ ìœ„í•´ ìƒì„±ëœ ë°ì´í„°ì…‹ (1000ê°œ, original)
- dpo_preference_dataset.jsonl : DPO í•™ìŠµ ìœ„í•´ ìƒì„±ëœ ë°ì´í„°ì…‹ (100ê°œ, original)
- dpo_preference_dataset_mitigated1000.jsonl : DPO í•™ìŠµ ìœ„í•´ ìƒì„±ëœ ë°ì´í„°ì…‹ (1000ê°œ, mitigated)
- dpo_preference_dataset_mitigated.jsonl : DPO í•™ìŠµ ìœ„í•´ ìƒì„±ëœ ë°ì´í„°ì…‹ (100ê°œ, mitigated)


- ğŸ“ **final_reward_adapters_length_bias** : length biasë¥¼ ê°–ëŠ” reward model adapter
- ğŸ“ **final_sft_adapters** : sft adapter
