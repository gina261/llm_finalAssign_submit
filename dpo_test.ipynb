{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13a904c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb0768fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.6.5: Fast Llama patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.433 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "==((====))==  Unsloth 2025.6.5: Fast Llama patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.433 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "base_model_id = \"unsloth/llama-3.2-1b-instruct-bnb-4bit\"\n",
    "dpo_adapter_path_original = os.path.abspath(\"./final_dpo_adapter_original\")\n",
    "dpo_adapter_path_mitigated = os.path.abspath(\"./final_dpo_adapter_mitigated\")\n",
    "\n",
    "# 베이스 모델 로드\n",
    "model_original, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=base_model_id,\n",
    "    max_seq_length=2048,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "model_mitigated, _ = FastLanguageModel.from_pretrained(\n",
    "    model_name=base_model_id,\n",
    "    max_seq_length=2048,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18f2cba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPO 어댑터를 로드\n",
      "DPO 어댑터 로딩 완료.\n"
     ]
    }
   ],
   "source": [
    "# DPO 어댑터 로드\n",
    "print(f\"DPO 어댑터를 로드\")\n",
    "model_original.load_adapter(dpo_adapter_path_original)\n",
    "model_mitigated.load_adapter(dpo_adapter_path_mitigated)\n",
    "print(\"DPO 어댑터 로딩 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00620d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== 추론 결과(original) =====\n",
      "what is carbon dioxide? Response: 2\n",
      "## Step 1: Definition of Carbon Dioxide\n",
      "Carbon dioxide (CO2) is a colorless, odorless, and tasteless gas that is a major component of the Earth's atmosphere. It is a simple, inorganic compound that is produced by the decomposition of carbon-based materials, such as wood and fossil fuels.\n",
      "\n",
      "## Step 2: Sources of Carbon Dioxide\n",
      "The primary sources of carbon dioxide are fossil fuel combustion, industrial processes, and natural gas seepage.\n",
      "===== 추론 결과(mitigated) =====\n",
      "what is carbon dioxide? Response: 1\n",
      "A. a colorless, odorless gas\n",
      "B. a colorless, odorless gas\n",
      "C. a colorless, odorless liquid\n",
      "D. a colorless, odorless solid\n",
      "Answer: B\n",
      "Explanation: Carbon dioxide is a colorless, odorless gas that is composed of carbon and oxygen atoms. It is the second most abundant gas in the Earth's atmosphere and is a major component of the atmosphere. It is also a key player in the carbon cycle, playing\n"
     ]
    }
   ],
   "source": [
    "# 추론 테스트\n",
    "prompt = \"what is carbon dioxide? Response: \"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs_original = model_original.generate(**inputs, max_new_tokens=100, use_cache=True)\n",
    "decoded_output_original = tokenizer.batch_decode(outputs_original, skip_special_tokens=True)[0]\n",
    "\n",
    "outputs_mitigated = model_mitigated.generate(**inputs, max_new_tokens=100, use_cache=True)\n",
    "decoded_output_mitigated = tokenizer.batch_decode(outputs_mitigated, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"===== 추론 결과(original) =====\")\n",
    "print(decoded_output_original)\n",
    "print(\"===== 추론 결과(mitigated) =====\")\n",
    "print(decoded_output_mitigated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cb4d3b",
   "metadata": {},
   "source": [
    "--------\n",
    "win rate 측정\n",
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a9a8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = \"YOUR_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "571de4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "import google.generativeai as genai\n",
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "307866e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 1. Configuration (User modification needed)\n",
    "# ==============================================================================\n",
    "# *** CRITICAL CHANGE ***\n",
    "# Use the newly created SFT-merged model as the base model.\n",
    "BASE_MODEL_ID = os.path.abspath(\"./sft_merged_model\") # 1단계에서 생성된 경로\n",
    "# SFT_ADAPTER_PATH is no longer needed here.\n",
    "DPO_ADAPTER_PATH_A = os.path.abspath(\"./final_dpo_adapter_original\")\n",
    "DPO_ADAPTER_PATH_B = os.path.abspath(\"./final_dpo_adapter_mitigated\")\n",
    "\n",
    "JUDGE_MODEL = \"gemini-2.0-flash-lite\"\n",
    "NUM_EVAL_PROMPTS = 50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6d5b366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 2. Load and Prepare Evaluation Dataset\n",
    "# ==============================================================================\n",
    "def load_and_prepare_dataset(num_samples):\n",
    "    \"\"\"Loads the mt_bench dataset from HuggingFaceH4 and extracts prompts for evaluation.\"\"\"\n",
    "    print(\"Loading HuggingFaceH4/mt_bench_prompts dataset...\")\n",
    "    \n",
    "    dataset_name = \"HuggingFaceH4/mt_bench_prompts\"\n",
    "    dataset = load_dataset(dataset_name)['train']\n",
    "    \n",
    "    prompts = []\n",
    "    for item in dataset:\n",
    "        # The 'prompt' column contains a list of strings; we need the first element for the first turn.\n",
    "        first_turn_prompt = item['prompt'][0]\n",
    "        # Format to match the Unsloth model's input structure\n",
    "        prompts.append(f\"Human: {first_turn_prompt}\\n\\nAssistant:\")\n",
    "    \n",
    "    # Randomly sample the specified number of prompts\n",
    "    if len(prompts) > num_samples:\n",
    "        print(f\"Randomly sampling {num_samples} prompts from the dataset.\")\n",
    "        return random.sample(prompts, num_samples)\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cafec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 3. Model Loading and Response Generation Functions\n",
    "# ==============================================================================\n",
    "def load_model_with_dpo_adapter(dpo_path):\n",
    "    \"\"\"\n",
    "    Loads the SFT-merged base model and applies a single DPO adapter on top.\n",
    "    \"\"\"\n",
    "    print(f\"Loading SFT-merged base model from: {BASE_MODEL_ID}\")\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=BASE_MODEL_ID, # Load the merged model\n",
    "        max_seq_length=2048,\n",
    "        dtype=None,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "    \n",
    "    print(f\"Loading DPO adapter: {os.path.basename(dpo_path)}\")\n",
    "    # SFT is already part of the model, so we only load the DPO adapter.\n",
    "    model.load_adapter(dpo_path)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_responses(prompts, model, tokenizer):\n",
    "    \"\"\"Generates responses for a given list of prompts using the specified model.\"\"\"\n",
    "    responses = []\n",
    "    for prompt in tqdm(prompts, desc=\"Generating responses\"):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = model.generate(**inputs, max_new_tokens=1024, use_cache=True, temperature=0.6, top_p=0.9)\n",
    "        response_with_prompt = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "        pure_response = response_with_prompt[len(prompt.replace(\"Assistant:\", \"\")):]\n",
    "        responses.append(pure_response.strip())\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6a1bd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 4. LLM-as-a-Judge (Gemini) Functions\n",
    "# ==============================================================================\n",
    "JUDGE_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an impartial evaluator assessing the responses of two AI assistants.\n",
    "Evaluate which of the two responses is more helpful, accurate, and better aligned with the user's intent.\n",
    "Don't be biased toward length.\n",
    "\n",
    "Consider the following criteria:\n",
    "1.  **Instruction Following**: Does the response accurately follow the user's instructions?\n",
    "2.  **Accuracy**: Is the information provided factually correct?\n",
    "3.  **Completeness**: Does the response provide sufficient information for the user's query?\n",
    "4.  **Conciseness & Fluency**: Is the response well-written, concise, and easy to understand?\n",
    "\n",
    "[User Prompt]\n",
    "{prompt}\n",
    "\n",
    "[Assistant A's Response]\n",
    "{response_a}\n",
    "\n",
    "[Assistant B's Response]\n",
    "{response_b}\n",
    "\n",
    "[Evaluation]\n",
    "Compare the two responses and briefly explain which one is better and why.\n",
    "After your explanation, state your final verdict on the last line in one of the following formats:\n",
    "- If Assistant A is better: [[A]]\n",
    "- If Assistant B is better: [[B]]\n",
    "- If the two responses are of similar quality or it's hard to decide: [[TIE]]\n",
    "\"\"\"\n",
    "\n",
    "def judge_battle_with_gemini(prompt, response_a, response_b):\n",
    "    \"\"\"Calls the Gemini judge model to determine the winner.\"\"\"\n",
    "    genai.configure(api_key=GOOGLE_API_KEY)\n",
    "    model = genai.GenerativeModel(JUDGE_MODEL)\n",
    "\n",
    "    is_swapped = random.choice([True, False])\n",
    "    if is_swapped:\n",
    "        res_a, res_b = response_b, response_a\n",
    "    else:\n",
    "        res_a, res_b = response_a, response_b\n",
    "\n",
    "    pure_prompt = prompt.replace(\"Human: \", \"\").replace(\"\\n\\nAssistant:\", \"\")\n",
    "    judge_prompt = JUDGE_PROMPT_TEMPLATE.format(prompt=pure_prompt, response_a=res_a, response_b=res_b)\n",
    "\n",
    "    for _ in range(3):  # Retry up to 3 times\n",
    "        try:\n",
    "            response = model.generate_content(judge_prompt)\n",
    "            content = response.text\n",
    "\n",
    "            if \"[[A]]\" in content:\n",
    "                return \"A\" if not is_swapped else \"B\", content\n",
    "            elif \"[[B]]\" in content:\n",
    "                return \"B\" if not is_swapped else \"A\", content\n",
    "            elif \"[[TIE]]\" in content:\n",
    "                return \"TIE\", content\n",
    "            else:\n",
    "                print(\"Warning: Judge model did not return a clear winner.\")\n",
    "                return \"BAD_RESPONSE\", content\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while calling the Gemini API: {e}\")\n",
    "            time.sleep(5)\n",
    "    return \"API_ERROR\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd2d6ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HuggingFaceH4/mt_bench_prompts dataset...\n",
      "Randomly sampling 50 prompts from the dataset.\n",
      "\n",
      "Loading Model A (SFT_merged + DPO_A)...\n",
      "Loading SFT-merged base model from: /nfs/home/ryan0507/checkllama/final/sft_merged_model\n",
      "==((====))==  Unsloth 2025.6.5: Fast Llama patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 10. Max memory: 23.433 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Loading DPO adapter: final_dpo_adapter_original\n",
      "\n",
      "Generating responses from Model A...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|██████████| 50/50 [03:12<00:00,  3.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Model B (SFT_merged + DPO_B)...\n",
      "Loading SFT-merged base model from: /nfs/home/ryan0507/checkllama/final/sft_merged_model\n",
      "==((====))==  Unsloth 2025.6.5: Fast Llama patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 10. Max memory: 23.433 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Loading DPO adapter: final_dpo_adapter_mitigated\n",
      "\n",
      "Generating responses from Model B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|██████████| 50/50 [02:51<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting evaluation with gemini-2.0-flash-lite...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Judging battles: 100%|██████████| 50/50 [01:39<00:00,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "          PERFORMANCE EVALUATION RESULTS\n",
      "==================================================\n",
      "Model A (Adapter: final_dpo_adapter_original)\n",
      "Model B (Adapter: final_dpo_adapter_mitigated)\n",
      "Judge Model: gemini-2.0-flash-lite\n",
      "Dataset: HuggingFaceH4/mt_bench_prompts (50 prompts)\n",
      "--------------------------------------------------\n",
      "Model A Win Rate: 56.00% (28 wins)\n",
      "Model B Win Rate: 40.00% (20 wins)\n",
      "Tie Rate: 4.00% (2 ties)\n",
      "==================================================\n",
      "\n",
      "Saving detailed battle log to 'battle_log_gemini_en.json'...\n",
      "Evaluation complete. Check 'battle_log_gemini_en.json' for detailed results.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 5. Main Execution Logic\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    evaluation_prompts = load_and_prepare_dataset(NUM_EVAL_PROMPTS)\n",
    "    \n",
    "    print(\"\\nLoading Model A (SFT_merged + DPO_A)...\")\n",
    "    model_a, tokenizer_a = load_model_with_dpo_adapter(DPO_ADAPTER_PATH_A)\n",
    "    print(\"\\nGenerating responses from Model A...\")\n",
    "    responses_a = generate_responses(evaluation_prompts, model_a, tokenizer_a)\n",
    "    \n",
    "    print(\"\\nLoading Model B (SFT_merged + DPO_B)...\")\n",
    "    model_b, tokenizer_b = load_model_with_dpo_adapter(DPO_ADAPTER_PATH_B)\n",
    "    print(\"\\nGenerating responses from Model B...\")\n",
    "    responses_b = generate_responses(evaluation_prompts, model_b, tokenizer_b)\n",
    "\n",
    "    # Clean up memory before starting the evaluation loop\n",
    "    del model_a, tokenizer_a, model_b, tokenizer_b\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"\\nStarting evaluation with {JUDGE_MODEL}...\")\n",
    "    results = {\"A_wins\": 0, \"B_wins\": 0, \"ties\": 0, \"errors\": 0}\n",
    "    battle_log = []\n",
    "\n",
    "    for i in tqdm(range(len(evaluation_prompts)), desc=\"Judging battles\"):\n",
    "        winner, judge_reason = judge_battle_with_gemini(evaluation_prompts[i], responses_a[i], responses_b[i])\n",
    "        \n",
    "        if winner == \"A\":\n",
    "            results[\"A_wins\"] += 1\n",
    "        elif winner == \"B\":\n",
    "            results[\"B_wins\"] += 1\n",
    "        elif winner == \"TIE\":\n",
    "            results[\"ties\"] += 1\n",
    "        else:\n",
    "            results[\"errors\"] += 1\n",
    "            \n",
    "        battle_log.append({\n",
    "            \"prompt\": evaluation_prompts[i],\n",
    "            \"response_A\": responses_a[i],\n",
    "            \"response_B\": responses_b[i],\n",
    "            \"winner\": winner,\n",
    "            \"judge_reason\": judge_reason,\n",
    "        })\n",
    "\n",
    "    # Print final results\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"          PERFORMANCE EVALUATION RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Model A (Adapter: {os.path.basename(DPO_ADAPTER_PATH_A)})\")\n",
    "    print(f\"Model B (Adapter: {os.path.basename(DPO_ADAPTER_PATH_B)})\")\n",
    "    print(f\"Judge Model: {JUDGE_MODEL}\")\n",
    "    print(f\"Dataset: HuggingFaceH4/mt_bench_prompts ({len(evaluation_prompts)} prompts)\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    total_judged = results[\"A_wins\"] + results[\"B_wins\"] + results[\"ties\"]\n",
    "    if total_judged > 0:\n",
    "        win_rate_a = (results[\"A_wins\"] / total_judged) * 100\n",
    "        win_rate_b = (results[\"B_wins\"] / total_judged) * 100\n",
    "        tie_rate = (results[\"ties\"] / total_judged) * 100\n",
    "        \n",
    "        print(f\"Model A Win Rate: {win_rate_a:.2f}% ({results['A_wins']} wins)\")\n",
    "        print(f\"Model B Win Rate: {win_rate_b:.2f}% ({results['B_wins']} wins)\")\n",
    "        print(f\"Tie Rate: {tie_rate:.2f}% ({results['ties']} ties)\")\n",
    "    \n",
    "    if results[\"errors\"] > 0:\n",
    "        print(f\"\\nErrors (API or Bad Response): {results['errors']}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Save detailed log to a file\n",
    "    print(\"\\nSaving detailed battle log to 'battle_log_gemini_en.json'...\")\n",
    "    with open(\"battle_log_gemini_en.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(battle_log, f, ensure_ascii=False, indent=4)\n",
    "    print(\"Evaluation complete. Check 'battle_log_gemini_en.json' for detailed results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3618beb",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc88478",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
