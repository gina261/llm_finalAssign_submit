import torch
from datasets import load_dataset
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    BitsAndBytesConfig,
)
from trl import RewardTrainer, RewardConfig
from peft import LoraConfig

# ====================================================================
# 1. ëª¨ë¸, í† í¬ë‚˜ì´ì €, ëª¨ë“  ì„¤ì • ì •ì˜
# ====================================================================

model_id = "unsloth/llama-3.2-1b-instruct-bnb-4bit"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="SEQ_CLS",
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
    modules_to_save=["score"],
)

training_args = RewardConfig(
    output_dir="./results_reward_model",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-5,
    max_steps=500,
    logging_steps=10,
    save_steps=100,
    fp16=True,
    eval_steps=100,
    push_to_hub=False,
    report_to="none",
    max_length=1024,
)

# ====================================================================
# 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ, ê·¸ë¦¬ê³  ë ˆì´ì–´ êµì²´
# ====================================================================

print("ğŸ’¾ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
model = AutoModelForSequenceClassification.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto",
    num_labels=1,
)

tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)

print("ğŸ”„ ë¬¸ì œê°€ ë˜ëŠ” 'score' ë ˆì´ì–´ë¥¼ êµì²´í•˜ê³  ì˜¬ë°”ë¥¸ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™í•©ë‹ˆë‹¤...")

in_features = model.score.in_features
new_score_layer = torch.nn.Linear(
    in_features,
    1,
    bias=False,
    dtype=bnb_config.bnb_4bit_compute_dtype
)

# ìƒˆë¡œ ë§Œë“  ë ˆì´ì–´ë¥¼ ëª¨ë¸ì˜ ë‚˜ë¨¸ì§€ ë¶€ë¶„ê³¼ ë™ì¼í•œ ë””ë°”ì´ìŠ¤(GPU)ë¡œ ì´ë™ì‹œí‚µë‹ˆë‹¤.
model.score = new_score_layer.to(model.device)

print("âœ… 'score' ë ˆì´ì–´ êµì²´ ë° ë””ë°”ì´ìŠ¤ ì´ë™ ì™„ë£Œ!")

model.config.use_cache = False
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    model.config.pad_token_id = tokenizer.pad_token_id

# ====================================================================
# 3. ë°ì´í„°ì…‹ ì¤€ë¹„ (Length Bias ì£¼ì…)
# ====================================================================
print("ğŸ’¾ ë³´ìƒ ëª¨ë¸ìš© ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
initial_data_size = 40000
dataset = load_dataset("Anthropic/hh-rlhf", split="train").select(range(initial_data_size))
print(f"ì›ë³¸ ë°ì´í„°ì…‹ í¬ê¸°: {len(dataset)}")

def filter_by_positive_length_bias(example):
    """
    'chosen' ì‘ë‹µì´ 'rejected' ì‘ë‹µë³´ë‹¤ ê¸´ ê²½ìš°ì—ë§Œ Trueë¥¼ ë°˜í™˜í•˜ëŠ” í•„í„°ë§ í•¨ìˆ˜ì…ë‹ˆë‹¤.
    """
    return len(example["chosen"]) > len(example["rejected"])

print("ğŸ”¥ ë°ì´í„°ì…‹ì—ì„œ 'chosen'ì´ 'rejected'ë³´ë‹¤ ê¸´ ë°ì´í„°ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤...")
filtered_dataset = dataset.filter(filter_by_positive_length_bias)
print(f"âœ… í•„í„°ë§ í›„ ë°ì´í„°ì…‹ í¬ê¸°: {len(filtered_dataset)} (ì›ë³¸ì˜ {len(filtered_dataset)/len(dataset):.2%} ìœ ì§€)")


print("âœ¨ ë°ì´í„°ì…‹ ì¸ë±ìŠ¤ë¥¼ í†µí•©í•˜ì—¬ ì•ˆì •ì„±ì„ ë†’ì…ë‹ˆë‹¤ (flatten_indices)...")
stable_dataset = filtered_dataset.flatten_indices()
print("âœ… ì¸ë±ìŠ¤ í†µí•© ì™„ë£Œ!")


# --- [ìˆ˜ì •ëœ ë¶€ë¶„] ---
# ë¶ˆí•„ìš”í•˜ê³  ëª¨ë“  ì»¬ëŸ¼ì„ ì‚­ì œí•˜ë˜ remove_columns ë¼ì¸ì„ ì™„ì „íˆ ì œê±°í•©ë‹ˆë‹¤.
# ì•ˆì •í™”ëœ 'stable_dataset'ì„ ì§ì ‘ ë¶„í• í•©ë‹ˆë‹¤.
print("ğŸ”ª ì•ˆì •í™”ëœ ë°ì´í„°ì…‹ì„ ë¶„í• í•©ë‹ˆë‹¤...")
processed_dataset = stable_dataset.train_test_split(test_size=0.1, seed=42)
train_dataset, eval_dataset = processed_dataset["train"], processed_dataset["test"]
# --- ìˆ˜ì • ì™„ë£Œ ---


# ë””ë²„ê¹…ì„ ìœ„í•´ ê° ë°ì´í„°ì…‹ì˜ í¬ê¸°ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
print(f"ë¶„í•  í›„ train_dataset í¬ê¸°: {len(train_dataset)}")
print(f"ë¶„í•  í›„ eval_dataset í¬ê¸°: {len(eval_dataset)}")

# eval_datasetì´ ë¹„ì–´ìˆì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ë°©ì–´ ì½”ë“œëŠ” ê·¸ëŒ€ë¡œ ë‘¡ë‹ˆë‹¤.
if len(eval_dataset) == 0:
    print("âš ï¸ í‰ê°€ ë°ì´í„°ì…‹ì´ ë¹„ì–´ìˆì–´, ì´ë²ˆ í•™ìŠµì—ì„œëŠ” í‰ê°€ë¥¼ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")
    eval_dataset = None
    training_args.evaluation_strategy = "no" # RewardConfigì—ì„œ ì´ ë¶€ë¶„ì„ "steps"ë¡œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.

print(f"âœ… ì´ {len(train_dataset)}ê°œì˜ ë°ì´í„°ë¡œ ë³´ìƒ ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")


# ====================================================================
# 4. í•™ìŠµ
# ===================================================================

print("ğŸš€ RewardTrainerë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
trainer = RewardTrainer(
    model=model,
    args=training_args,
    processing_class=tokenizer,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    peft_config=lora_config,
)

print("ğŸš€ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
trainer.train()

print("ğŸ’¾ í•™ìŠµëœ ëª¨ë¸(ì–´ëŒ‘í„°)ì„ ì €ì¥í•©ë‹ˆë‹¤...")
trainer.save_model("./final_reward_adapters_length_bias")

print("ğŸ‰ í•™ìŠµì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")