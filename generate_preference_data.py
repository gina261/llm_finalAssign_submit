import torch
import pandas as pd
from datasets import load_dataset
from transformers import (
    AutoModelForSequenceClassification,
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
)
from peft import PeftModel
from tqdm import tqdm

# ====================================================================
# 1. ê²½ë¡œ ë° ì„¤ì • ì •ì˜
# ====================================================================
# unsloth ëª¨ë¸ì„ ê³„ì† ì‚¬ìš©í•©ë‹ˆë‹¤.
base_model_id = "unsloth/llama-3.2-1b-instruct-bnb-4bit"
sft_adapter_path = "./final_sft_adapters"
reward_adapter_path = "./final_reward_adapters_length_bias"
output_dataset_path = "./dpo/dpo_preference_dataset_2000.jsonl"

NUM_SAMPLES = 1000
device = "cuda:0" if torch.cuda.is_available() else "cpu"
print(f"ì‚¬ìš© ì¥ì¹˜(Device): {device}")

# 4ë¹„íŠ¸ ì–‘ìí™” ì„¤ì •
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

# ====================================================================
# 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
# ====================================================================
print("ğŸ’¾ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
tokenizer = AutoTokenizer.from_pretrained(base_model_id)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# SFT ëª¨ë¸ ë¡œë“œ (CausalLMì€ 'score' ë ˆì´ì–´ê°€ ì—†ì–´ ë¬¸ì œê°€ ì—†ìœ¼ë¯€ë¡œ, ì•ˆì •ì ìœ¼ë¡œ ë¡œë“œ)
print("ğŸ’¾ ì‘ë‹µ ìƒì„±ì„ ìœ„í•œ SFT ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
sft_model = AutoModelForCausalLM.from_pretrained(
    base_model_id,
    quantization_config=bnb_config,
    device_map={"": device} # device_map="auto" ëŒ€ì‹  ëª…ì‹œì ìœ¼ë¡œ ë‹¨ì¼ ì¥ì¹˜ í• ë‹¹
)
sft_model = PeftModel.from_pretrained(sft_model, sft_adapter_path)
sft_model.eval()


# --- Reward ëª¨ë¸ ë¡œë“œ (í•µì‹¬ì ì¸ 'ë ˆì´ì–´ êµì²´' í•´ê²°ë²• ì ìš©) ---
print("ğŸ’¾ Reward ëª¨ë¸ ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

# 1. ë² ì´ìŠ¤ ëª¨ë¸ì„ ìš°ì„  ë¡œë“œí•©ë‹ˆë‹¤. (ì•„ì§ GPUë¡œ ë³´ë‚´ì§€ ì•ŠìŒ)
base_reward_model = AutoModelForSequenceClassification.from_pretrained(
    base_model_id,
    quantization_config=bnb_config,
    num_labels=1,
)

# 2. ë¬¸ì œê°€ ë˜ëŠ” 'score' ë ˆì´ì–´ë¥¼ ê¹¨ë—í•œ ìƒˆ ë ˆì´ì–´ë¡œ êµì²´í•©ë‹ˆë‹¤.
print("ğŸ”„ ë¬¸ì œê°€ ë˜ëŠ” 'score' ë ˆì´ì–´ë¥¼ êµì²´í•©ë‹ˆë‹¤...")
in_features = base_reward_model.score.in_features
new_score_layer = torch.nn.Linear(
    in_features=in_features,
    out_features=1,
    bias=False,
    dtype=torch.float16
)
base_reward_model.score = new_score_layer
print("âœ… 'score' ë ˆì´ì–´ êµì²´ ì™„ë£Œ!")

# 3. ë ˆì´ì–´ êµì²´ê°€ ì™„ë£Œëœ ëª¨ë¸ì„ GPUë¡œ ì´ë™ì‹œí‚µë‹ˆë‹¤.
print(f"ğŸ”„ ìˆ˜ì •ëœ Reward ë² ì´ìŠ¤ ëª¨ë¸ì„ {device}ë¡œ ì´ë™í•©ë‹ˆë‹¤...")
base_reward_model.to(device)

# 4. ì•ˆì •í™”ëœ ë² ì´ìŠ¤ ëª¨ë¸ ìœ„ì— Reward ì–´ëŒ‘í„°ë¥¼ ì ìš©í•©ë‹ˆë‹¤.
print(f"ğŸ”„ '{reward_adapter_path}'ì˜ ì–´ëŒ‘í„°ë¥¼ ì ìš©í•©ë‹ˆë‹¤...")
reward_model = PeftModel.from_pretrained(base_reward_model, reward_adapter_path)
reward_model.eval()

print("ğŸ‰ ëª¨ë“  ëª¨ë¸ ë¡œë”© ë° ìˆ˜ì • ì™„ë£Œ!")


# ====================================================================
# 3. ë°ì´í„°ì…‹ ìƒì„± (ì´ì œ ì •ìƒì ìœ¼ë¡œ ë™ì‘í•´ì•¼ í•©ë‹ˆë‹¤)
# ====================================================================
print(f"ğŸ’¾ í”„ë¡¬í”„íŠ¸ ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ì—¬ {NUM_SAMPLES}ê°œì˜ ì„ í˜¸ë„ ìŒì„ ìƒì„±í•©ë‹ˆë‹¤...")
dataset = load_dataset("Anthropic/hh-rlhf", split="train").select(range(50000, 50000+NUM_SAMPLES))

preference_data = []

generation_kwargs = {
    "top_p": 0.9,
    "do_sample": True,
    "pad_token_id": tokenizer.pad_token_id,
    "max_new_tokens": 128,
    "temperature": 0.7,
}

for sample in tqdm(dataset, desc="ì„ í˜¸ë„ ë°ì´í„°ì…‹ ìƒì„± ì¤‘"):
    full_text = sample["chosen"]
    prompt_end_str = "\n\nAssistant:"
    prompt_start_index = full_text.find("Human:")
    prompt_end_index = full_text.find(prompt_end_str)
    if prompt_start_index == -1 or prompt_end_index == -1:
        continue
    prompt_text = full_text[:prompt_end_index + len(prompt_end_str)]

    input_ids = tokenizer(prompt_text, return_tensors="pt").input_ids.to(device)

    # ì‘ë‹µ ìƒì„±
    response_1_ids = sft_model.generate(input_ids, **generation_kwargs)
    response_2_ids = sft_model.generate(input_ids, **generation_kwargs)

    response_1_text = tokenizer.decode(response_1_ids[0][len(input_ids[0]):], skip_special_tokens=True)
    response_2_text = tokenizer.decode(response_2_ids[0][len(input_ids[0]):], skip_special_tokens=True)

    if response_1_text == response_2_text:
        continue

    full_text_1 = prompt_text + response_1_text
    full_text_2 = prompt_text + response_2_text

    # ë‘ ì‘ë‹µ í‰ê°€
    reward_inputs = tokenizer([full_text_1, full_text_2], padding=True, truncation=True, max_length=1024, return_tensors="pt").to(device)

    with torch.no_grad():
        scores = reward_model(**reward_inputs).logits.squeeze()

    # squeeze ê²°ê³¼ê°€ ë‹¨ì¼ ìŠ¤ì¹¼ë¼ í…ì„œì¼ ê²½ìš°ë¥¼ ëŒ€ë¹„
    if scores.dim() == 0:
        scores = scores.unsqueeze(0)
        
    # ë§Œì•½ ë°°ì¹˜ ì‚¬ì´ì¦ˆ 1ì—ì„œ ë‘ ê°œì˜ ì‘ë‹µì„ ìƒì„±í•˜ì—¬ ì ìˆ˜ê°€ í•˜ë‚˜ë§Œ ë‚˜ì˜¨ ê²½ìš° (ì˜¤ë¥˜ ë°©ì§€)
    if len(scores) < 2:
        continue
        
    score_1, score_2 = scores[0].item(), scores[1].item()

    if score_1 > score_2:
        chosen_response = full_text_1
        rejected_response = full_text_2
    else:
        chosen_response = full_text_2
        rejected_response = full_text_1

    preference_data.append({
        "prompt": prompt_text,
        "chosen": chosen_response,
        "rejected": rejected_response
    })

df = pd.DataFrame(preference_data)
df.to_json(output_dataset_path, orient='records', lines=True)

print(f"âœ… ì´ {len(df)}ê°œì˜ ì„ í˜¸ë„ ë°ì´í„° ìƒì„± ì™„ë£Œ! íŒŒì¼: '{output_dataset_path}'")