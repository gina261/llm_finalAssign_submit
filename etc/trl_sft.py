import torch
from datasets import load_dataset, concatenate_datasets, Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
)
from peft import LoraConfig
from trl import SFTTrainer


# 1. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì„¤ì •
# Unslothì—ì„œ ì œê³µí•˜ëŠ” 4ë¹„íŠ¸ ì–‘ìí™”ëœ Llama-3.2-1B ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
# ê³µì‹ ëª¨ë¸ì´ ë‚˜ì˜¤ë©´ í•´ë‹¹ IDë¡œ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
model_id = "unsloth/llama-3.2-1b-instruct-bnb-4bit"

# 4ë¹„íŠ¸ ì–‘ìí™” ì„¤ì • (QLoRA)
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

# LoRA ì„¤ì •
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    # Llama3.2 ëª¨ë¸ì˜ ì–´í…ì…˜ ë° í”¼ë“œí¬ì›Œë“œ ë ˆì´ì–´ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì§€ì •
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
)


# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto",
)
model.config.use_cache = False # í•™ìŠµ ì‹œì—ëŠ” ìºì‹œ ë¹„í™œì„±í™”

tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token # PAD í† í° ì„¤ì •
    
# 2. ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬ (ë¹„ìœ¨ ì¡°ì • ì ìš©)

# =================== ë¹„ìœ¨ ì„¤ì • ===================
TARGET_TOTAL_SAMPLES = 20000  # í•™ìŠµì— ì‚¬ìš©í•  ì´ ë°ì´í„° ìˆ˜
ALPACA_RATIO = 0.7            # Alpaca ë°ì´í„°ì…‹ì˜ ë¹„ìœ¨ (0.0 ~ 1.0)
GSM8K_RATIO = 1.0 - ALPACA_RATIO
# ===============================================

def preprocess_alpaca(examples):
    # ... (ì´ì „ ì½”ë“œì™€ ë™ì¼)
    processed_examples = []
    for instruction, input_text, output in zip(examples['instruction'], examples['input'], examples['output']):
        user_content = f"{instruction}\n\n{input_text}" if input_text else instruction
        processed_examples.append({"messages": [{"role": "user", "content": user_content}, {"role": "assistant", "content": output}]})
    return {"messages": [example["messages"] for example in processed_examples]}

def preprocess_gsm8k(examples):
    # ... (ì´ì „ ì½”ë“œì™€ ë™ì¼)
    processed_examples = []
    for question, answer in zip(examples['question'], examples['answer']):
        processed_examples.append({"messages": [{"role": "user", "content": question}, {"role": "assistant", "content": answer}]})
    return {"messages": [example["messages"] for example in processed_examples]}

# ë°ì´í„°ì…‹ ë¡œë“œ
print("ğŸ’¾ ë°ì´í„°ì…‹ ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
alpaca_dataset = load_dataset("yahma/alpaca-cleaned", split="train")
gsm8k_dataset = load_dataset("gsm8k", "main", split="train")

print(f"ì›ë³¸ ë°ì´í„° í¬ê¸°: Alpaca={len(alpaca_dataset)}, GSM8K={len(gsm8k_dataset)}")

# ê° ë°ì´í„°ì…‹ì—ì„œ ìƒ˜í”Œë§í•  ê°œìˆ˜ ê³„ì‚°
num_alpaca_samples = int(TARGET_TOTAL_SAMPLES * ALPACA_RATIO)
num_gsm8k_samples = int(TARGET_TOTAL_SAMPLES * GSM8K_RATIO)

# ì›ë³¸ ë°ì´í„°ì…‹ í¬ê¸°ë³´ë‹¤ ë§ì´ ìƒ˜í”Œë§í•  ìˆ˜ ì—†ë„ë¡ ì¡°ì •
num_alpaca_samples = min(num_alpaca_samples, len(alpaca_dataset))
num_gsm8k_samples = min(num_gsm8k_samples, len(gsm8k_dataset))

print(f"ìƒ˜í”Œë§í•  ë°ì´í„° í¬ê¸°: Alpaca={num_alpaca_samples}, GSM8K={num_gsm8k_samples}")

# ë°ì´í„°ì…‹ì„ ì„ê³  í•„ìš”í•œ ë§Œí¼ ìƒ˜í”Œë§
# .shuffle()ì„ ë¨¼ì € í•´ì•¼ ë¬´ì‘ìœ„ ìƒ˜í”Œì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
alpaca_sampled = alpaca_dataset.shuffle(seed=42).select(range(num_alpaca_samples))
gsm8k_sampled = gsm8k_dataset.shuffle(seed=42).select(range(num_gsm8k_samples))

# ì „ì²˜ë¦¬ ì ìš©
print("ğŸ”„ ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤...")
alpaca_processed = alpaca_sampled.map(lambda examples: preprocess_alpaca(examples), batched=True, remove_columns=alpaca_sampled.column_names)
gsm8k_processed = gsm8k_sampled.map(lambda examples: preprocess_gsm8k(examples), batched=True, remove_columns=gsm8k_sampled.column_names)

# ë‘ ë°ì´í„°ì…‹ ë³‘í•©
print("ğŸ¤ ë°ì´í„°ì…‹ì„ ë³‘í•©í•˜ê³  ì…”í”Œí•©ë‹ˆë‹¤...")
combined_dataset = concatenate_datasets([alpaca_processed, gsm8k_processed])
combined_dataset = combined_dataset.shuffle(seed=42)

print(f"âœ… ì´ {len(combined_dataset)}ê°œì˜ ë°ì´í„°ë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
print("ìƒ˜í”Œ ë°ì´í„°:", combined_dataset[0]['messages'])


# 3. SFTTrainer ì„¤ì • ë° í•™ìŠµ
# í•™ìŠµ ì¸ì(Arguments) ì„¤ì •
training_args = TrainingArguments(
    output_dir="./results_sft_mixed",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    max_steps=1000,  # í•„ìš”ì— ë”°ë¼ ì´ í•™ìŠµ ìŠ¤í… ìˆ˜ ì¡°ì •
    logging_steps=10,
    save_steps=100,
    fp16=True, # bfloat16ì„ ì“¸ ìˆ˜ ì—†ëŠ” í™˜ê²½ì—ì„œëŠ” fp16 ì‚¬ìš©
    push_to_hub=False,
    report_to="none", # wandb ë“± ë¡œê¹… ë„êµ¬ ì‚¬ìš© ì‹œ "wandb"ë¡œ ë³€ê²½
)

# SFTTrainer ì´ˆê¸°í™”
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=combined_dataset,
    peft_config=lora_config,
)

# í•™ìŠµ ì‹œì‘
print("ğŸš€ SFT í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
trainer.train()

# í•™ìŠµ ì™„ë£Œ í›„ ëª¨ë¸ ì €ì¥
print("ğŸ’¾ í•™ìŠµëœ ëª¨ë¸(ì–´ëŒ‘í„°)ì„ ì €ì¥í•©ë‹ˆë‹¤...")
trainer.save_model("./final_sft_adapters")

print("ğŸ‰ í•™ìŠµì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")