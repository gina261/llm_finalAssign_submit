import torch
from datasets import load_dataset
from transformers import (
    AutoModelForSequenceClassification,
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    DataCollatorWithPadding,
)
# Value Headê°€ í¬í•¨ëœ ëª¨ë¸ì„ ë‹¤ì‹œ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ êµ¬ì¡°ì˜ í•µì‹¬ì…ë‹ˆë‹¤.
from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead
from peft import LoraConfig, PeftModel
from tqdm import tqdm

# ====================================================================
# 1. ëª¨ë“  ê²½ë¡œ ë° ì„¤ì • ì •ì˜
# ====================================================================

# SFT ì–´ëŒ‘í„° ê²½ë¡œ
sft_adapter_path = "./final_sft_adapter"
# -------------------------

base_model_id = "unsloth/llama-3.2-1b-instruct-bnb-4bit"
reward_adapter_path = "./final_reward_adapters_length_bias"
rlhf_adapter_path = "./final_rlhf_adapter_length_bias" # ìµœì¢…ì ìœ¼ë¡œ ì €ì¥ë  RLHF ì–´ëŒ‘í„° ê²½ë¡œ

# PPO í•™ìŠµì„ ìœ„í•œ ì„¤ì •
ppo_config = PPOConfig(
    learning_rate=1.41e-5,
    batch_size=16,
    mini_batch_size=4,
    gradient_accumulation_steps=1,
    kl_coef=0.1,
    num_ppo_epochs=4,
    seed=42,
)

# RLHF ê³¼ì •ì—ì„œ Policy ëª¨ë¸ì„ íŠœë‹í•˜ê¸° ìœ„í•œ LoRA ì„¤ì •
# SFT ë•Œ ì‚¬ìš©í–ˆë˜ ì„¤ì •ê³¼ ë™ì¼í•˜ê²Œ ë§ì¶”ëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤.
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM", # RLHFëŠ” Causal LMì´ë¯€ë¡œ "CAUSAL_LM"ìœ¼ë¡œ ì„¤ì •
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
)

# 4-bit ì–‘ìí™” ì„¤ì •
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

# ====================================================================
# 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
# ====================================================================
print("ğŸ’¾ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
tokenizer = AutoTokenizer.from_pretrained(base_model_id)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

print("ğŸ’¾ í•™ìŠµ ëŒ€ìƒ Policy ëª¨ë¸(Actor)ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
policy_model = AutoModelForCausalLM.from_pretrained(
    base_model_id,
    quantization_config=bnb_config,
    device_map="auto",
)
policy_model = PeftModel(policy_model, lora_config)

print("ğŸ’¾ ì°¸ì¡°(Ref) ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
ref_model = AutoModelForCausalLM.from_pretrained(
    base_model_id,
    quantization_config=bnb_config,
    device_map="auto",
)
ref_model = PeftModel(ref_model, lora_config)

print("ğŸ’¾ ê°€ì¹˜(Value) ëª¨ë¸(Critic)ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
value_model = AutoModelForCausalLM.from_pretrained(
    base_model_id,
    quantization_config=bnb_config,
    device_map="auto",
)
value_model = PeftModel(value_model, lora_config)

print("ğŸ’¾ í‰ê°€ìš© Reward ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
reward_model = AutoModelForSequenceClassification.from_pretrained(
    base_model_id,
    quantization_config=bnb_config,
    device_map="auto",
    num_labels=1,
)
reward_model = PeftModel.from_pretrained(reward_model, reward_adapter_path)

# [ìµœì¢… ìˆ˜ì •] LoRA ì–´ëŒ‘í„°ë¥¼ ì›ë³¸ ëª¨ë¸ì— ë³‘í•©í•˜ê³  PEFT ë˜í¼ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
print("âœ¨ Reward ëª¨ë¸ì˜ LoRA ì–´ëŒ‘í„°ë¥¼ ë³‘í•©í•©ë‹ˆë‹¤...")
reward_model = reward_model.merge_and_unload()
print("âœ… ë³‘í•© ì™„ë£Œ!")


reward_model.eval()
print("âœ… ëª¨ë“  ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")


# ====================================================================
# 3. ë°ì´í„°ì…‹ ì¤€ë¹„ (RLHFìš© í”„ë¡¬í”„íŠ¸)
# ====================================================================
print("ğŸ’¾ RLHFìš© í”„ë¡¬í”„íŠ¸ ë°ì´í„°ì…‹ì„ ì¤€ë¹„í•©ë‹ˆë‹¤...")
def create_prompt(sample):
    prompt_start = "\n\nHuman: "
    prompt_end = "\n\nAssistant:"
    full_text = sample["chosen"]
    start_index = full_text.find(prompt_start)
    end_index = full_text.find(prompt_end, start_index)
    if start_index != -1 and end_index != -1:
        prompt = full_text[start_index + len(prompt_start):end_index].strip()
        sample["query"] = f"{prompt_start}{prompt}{prompt_end}"
    else:
        sample["query"] = ""
    return sample

dataset = load_dataset("Anthropic/hh-rlhf", split="train").select(range(50000, 50100))
dataset = dataset.map(create_prompt).filter(lambda x: len(x["query"]) > 0)

# PPOTrainer ë‚´ë¶€ì—ì„œ ì‚¬ìš©í•  í† í¬ë‚˜ì´ì§• í•¨ìˆ˜
def tokenize_function(examples):
    return tokenizer(examples["query"], truncation=True)

print("âœ¨ ë°ì´í„°ì…‹ì„ í† í¬ë‚˜ì´ì§•í•˜ê³  ì •ë¦¬í•©ë‹ˆë‹¤...")
tokenized_dataset = dataset.map(
    tokenize_function,
    batched=True,
    # [ìµœì¢… ìˆ˜ì •] 'input_ids'ì™€ 'attention_mask'ë¥¼ ì œì™¸í•œ ëª¨ë“  ì›ë³¸ ì»¬ëŸ¼ì„ ëª…ì‹œì ìœ¼ë¡œ ì‚­ì œí•©ë‹ˆë‹¤.
    remove_columns=['chosen', 'rejected']
)
tokenized_dataset.set_format("torch")
print("âœ… í† í¬ë‚˜ì´ì§• ë° ì •ë¦¬ ì™„ë£Œ!")

# ====================================================================
# 4. PPOTrainer ì´ˆê¸°í™” ë° í•™ìŠµ
# ====================================================================
# [ìƒˆë¡œìš´ ì½”ë“œ ì¶”ê°€] ì‚¬ìš©ì ì •ì˜ ë°ì´í„° ì½œë ˆì´í„° ì •ì˜
class DataCollatorForPPO:
    def __init__(self, base_collator):
        self.base_collator = base_collator

    def __call__(self, features):
        # 'query' ë¬¸ìì—´ì„ ë¶„ë¦¬í•©ë‹ˆë‹¤.
        # .pop()ì€ ë”•ì…”ë„ˆë¦¬ì—ì„œ í•´ë‹¹ í‚¤ë¥¼ ì œê±°í•˜ê³  ê·¸ ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        queries = [feature.pop("query") for feature in features]
        
        # ë‚˜ë¨¸ì§€ ìˆ«ì ë°ì´í„°(input_ids, attention_mask)ë§Œ ê¸°ë³¸ ì½œë ˆì´í„°ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        batch = self.base_collator(features)
        
        # ë¶„ë¦¬í–ˆë˜ 'query'ë¥¼ ë‹¤ì‹œ ë°°ì¹˜ì— ì¶”ê°€í•©ë‹ˆë‹¤.
        batch["query"] = queries
        
        return batch

print("ğŸš€ PPOTrainerë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")

data_collator = DataCollatorForPPO(DataCollatorWithPadding(tokenizer=tokenizer))

# PPOTrainerëŠ” ìµœì†Œí•œì˜ ìš”ì†Œë§Œ ë°›ìŠµë‹ˆë‹¤.
ppo_trainer = PPOTrainer(
    args=ppo_config,              # 'config'ê°€ ì•„ë‹Œ 'args' ì‚¬ìš©
    model=policy_model,
    ref_model=ref_model,
    reward_model=reward_model,
    value_model=value_model,
    processing_class=tokenizer,   # 'tokenizer'ê°€ ì•„ë‹Œ 'processing_class' ì‚¬ìš©
    train_dataset=tokenized_dataset, # 'dataset'ì´ ì•„ë‹Œ 'train_dataset' ì‚¬ìš© ë° í† í¬ë‚˜ì´ì§•ëœ ë°ì´í„° ì „ë‹¬
    data_collator=data_collator,
)

generation_kwargs = {
    "min_length": -1,
    "top_k": 0.0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.pad_token_id,
    "max_new_tokens": 128,
}

print("ğŸš€ RLHF í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤ (ëª…ì‹œì  ë£¨í”„ ì‚¬ìš©)...")

for epoch in range(ppo_config.num_ppo_epochs):
    print(f"Epoch {epoch+1}/{ppo_config.num_ppo_epochs}")
    for batch in tqdm(ppo_trainer.dataloader):
        query_tensors = batch["input_ids"]

        # 1. [ìµœì¢… ìˆ˜ì •] policy_modelì—ì„œ ì§ì ‘ ì‘ë‹µ ìƒì„±
        #    ì´ê²ƒì´ ê°€ì¥ í™•ì‹¤í•˜ê³  í‘œì¤€ì ì¸ ë°©ë²•ì…ë‹ˆë‹¤.
        #    ìƒì„± ê²°ê³¼ëŠ” 'í”„ë¡¬í”„íŠ¸ + ì‘ë‹µ' í…ì„œì…ë‹ˆë‹¤.
        response_tensors = policy_model.generate(query_tensors, **generation_kwargs)

        # 2. [ìµœì¢… ìˆ˜ì •] ë³´ìƒ ê³„ì‚°ì„ ìœ„í•´ ìˆœìˆ˜ 'ì‘ë‹µ' ë¶€ë¶„ë§Œ ë¶„ë¦¬
        #    response_tensorsì—ì„œ query_tensors ë¶€ë¶„ì„ ì˜ë¼ëƒ…ë‹ˆë‹¤.
        batch["response"] = [
            tokenizer.decode(response[len(query):], skip_special_tokens=True)
            for query, response in zip(query_tensors, response_tensors)
        ]
        
        # ì´ì œ texts_for_rewardëŠ” 'í”„ë¡¬í”„íŠ¸(query_text) + ì‘ë‹µ(response_text)'ë¡œ ì˜¬ë°”ë¥´ê²Œ ì¡°í•©ë©ë‹ˆë‹¤.
        texts_for_reward = [q + r for q, r in zip(batch['query'], batch['response'])]
        
        reward_inputs = tokenizer(texts_for_reward, padding=True, truncation=True, max_length=1024, return_tensors="pt").to(reward_model.device)
        
        with torch.no_grad():
            rewards = reward_model(**reward_inputs).logits.squeeze(-1)
            
        # 3. PPO ì—…ë°ì´íŠ¸ ë‹¨ê³„ ìˆ˜í–‰
        #    step í•¨ìˆ˜ì—ëŠ” 'í”„ë¡¬í”„íŠ¸'ì™€ 'í”„ë¡¬í”„íŠ¸+ì‘ë‹µ' í…ì„œë¥¼ ê·¸ëŒ€ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.
        #    PPOTrainerëŠ” ë‚´ë¶€ì ìœ¼ë¡œ ì´ ë‘˜ì„ ë¹„êµí•˜ì—¬ KL í˜ë„í‹° ë“±ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
        ppo_trainer.log_stats(stats, batch, rewards.cpu().numpy())


print("âœ… RLHF í•™ìŠµ ì™„ë£Œ!")

# ====================================================================
# 5. ìµœì¢… ëª¨ë¸ ì €ì¥
# ====================================================================
print(f"ğŸ’¾ ìµœì¢… RLHF ì–´ëŒ‘í„°ë¥¼ '{rlhf_adapter_path}'ì— ì €ì¥í•©ë‹ˆë‹¤...")
ppo_trainer.save_pretrained(rlhf_adapter_path)
print("ğŸ‰ ëª¨ë“  ê³¼ì •ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")