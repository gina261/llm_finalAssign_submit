import torch
from datasets import load_dataset
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    BitsAndBytesConfig,
)
from trl import RewardTrainer, RewardConfig
from peft import LoraConfig

# ====================================================================
# 1. ëª¨ë¸, í† í¬ë‚˜ì´ì €, ëª¨ë“  ì„¤ì • ì •ì˜
# ====================================================================

model_id = "unsloth/llama-3.2-1b-instruct-bnb-4bit"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="SEQ_CLS",
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
    modules_to_save=["score"],
)

training_args = RewardConfig(
    output_dir="./results_reward_model",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-5,
    max_steps=500,
    logging_steps=10,
    save_steps=100,
    fp16=True,
    eval_steps=100,
    push_to_hub=False,
    report_to="none",
    max_length=1024,
)

# ====================================================================
# 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ, ê·¸ë¦¬ê³  ë ˆì´ì–´ êµì²´
# ====================================================================

print("ğŸ’¾ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
model = AutoModelForSequenceClassification.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto",
    num_labels=1,
)

tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)

# ==================== ìµœì¢… ìˆ˜ì • ì½”ë“œ ====================
print("ğŸ”„ ë¬¸ì œê°€ ë˜ëŠ” 'score' ë ˆì´ì–´ë¥¼ êµì²´í•˜ê³  ì˜¬ë°”ë¥¸ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™í•©ë‹ˆë‹¤...")

in_features = model.score.in_features
new_score_layer = torch.nn.Linear(
    in_features,
    1,
    bias=False,
    dtype=bnb_config.bnb_4bit_compute_dtype
)

# ìƒˆë¡œ ë§Œë“  ë ˆì´ì–´ë¥¼ ëª¨ë¸ì˜ ë‚˜ë¨¸ì§€ ë¶€ë¶„ê³¼ ë™ì¼í•œ ë””ë°”ì´ìŠ¤(GPU)ë¡œ ì´ë™ì‹œí‚µë‹ˆë‹¤.
model.score = new_score_layer.to(model.device) # <--- ì´ í•œ ì¤„ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!

print("âœ… 'score' ë ˆì´ì–´ êµì²´ ë° ë””ë°”ì´ìŠ¤ ì´ë™ ì™„ë£Œ!")
# ========================================================

model.config.use_cache = False
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    model.config.pad_token_id = tokenizer.pad_token_id

# ====================================================================
# 3. ë°ì´í„°ì…‹ ì¤€ë¹„ ë° 4. í•™ìŠµ (ì´í•˜ ì™„ì „ ë™ì¼)
# ====================================================================
print("ğŸ’¾ ë³´ìƒ ëª¨ë¸ìš© ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•©ë‹ˆë‹¤...")
dataset = load_dataset("Anthropic/hh-rlhf", split="train").select(range(20000))

def preprocess_reward_dataset(examples):
    return {"chosen": examples["chosen"], "rejected": examples["rejected"]}

processed_dataset = dataset.map(preprocess_reward_dataset, batched=True, remove_columns=dataset.column_names)
processed_dataset = processed_dataset.train_test_split(test_size=0.1, seed=42)
train_dataset, eval_dataset = processed_dataset["train"], processed_dataset["test"]

print(f"âœ… ì´ {len(train_dataset)}ê°œì˜ ë°ì´í„°ë¡œ ë³´ìƒ ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")

print("ğŸš€ RewardTrainerë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤...")
trainer = RewardTrainer(
    model=model,
    args=training_args,
    processing_class=tokenizer,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    peft_config=lora_config,
)

print("ğŸš€ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
trainer.train()

print("ğŸ’¾ í•™ìŠµëœ ëª¨ë¸(ì–´ëŒ‘í„°)ì„ ì €ì¥í•©ë‹ˆë‹¤...")
trainer.save_model("./final_reward_adapters_data20000")

print("ğŸ‰ í•™ìŠµì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")